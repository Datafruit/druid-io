---
layout: doc_page
---
流拉摄取
=========================

如果你想从外部服务拉数据，你有两个选择。最简单的选择是设置“复制”从数据源读取的服务和使用[流推方法](#stream-push)写到Druid。

另一选择是*流拉*。使用这个方法，Druid实时节点从[Firehose](../ingestion/firehose.html)摄取数据连接到你想要读的数据。
Druid快速入门和教程不包括怎么设置独立的实时节点，但是它可以适当的使用于Tranguility服务和索引服务。请注意实时节点比索引服务有更多的属性。



## 实时节点摄取

在这个页面上，多大数配置管理的实时节点和数据集的摄入是设置在实时规范文件中的。
对于一般实时节点信息，查阅[这里](../design/realtime.html)。
对于实时节点配置，查阅[实时配置](../configuration/realtime.html)。
对于写你的组件到实时节点，查阅[Firehose](../ingestion/firehose.html)。

## Realtime "specFile"

`druid.realtime.specFile`文件有一个文件的路径（绝对路径或者相对路径和文件名）有实时规范。这个“specFile”应该是一个JSON对象的一个JSON数组，如以下：
```json
[
  {
    "dataSchema" : {
      "dataSource" : "wikipedia",
      "parser" : {
        "type" : "string",
        "parseSpec" : {
          "format" : "json",
          "timestampSpec" : {
            "column" : "timestamp",
            "format" : "auto"
          },
          "dimensionsSpec" : {
            "dimensions": ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"],
            "dimensionExclusions" : [],
            "spatialDimensions" : []
          }
        }
      },
      "metricsSpec" : [{
        "type" : "count",
        "name" : "count"
      }, {
        "type" : "doubleSum",
        "name" : "added",
        "fieldName" : "added"
      }, {
        "type" : "doubleSum",
        "name" : "deleted",
        "fieldName" : "deleted"
      }, {
        "type" : "doubleSum",
        "name" : "delta",
        "fieldName" : "delta"
      }],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "DAY",
        "queryGranularity" : "NONE"
      }
    },
    "ioConfig" : {
      "type" : "realtime",
      "firehose": {
        "type": "kafka-0.8",
        "consumerProps": {
          "zookeeper.connect": "localhost:2181",
          "zookeeper.connection.timeout.ms" : "15000",
          "zookeeper.session.timeout.ms" : "15000",
          "zookeeper.sync.time.ms" : "5000",
          "group.id": "druid-example",
          "fetch.message.max.bytes" : "1048586",
          "auto.offset.reset": "largest",
          "auto.commit.enable": "false"
        },
        "feed": "wikipedia"
      },
      "plumber": {
        "type": "realtime"
      }
    },
    "tuningConfig": {
      "type" : "realtime",
      "maxRowsInMemory": 500000,
      "intermediatePersistPeriod": "PT10m",
      "windowPeriod": "PT10m",
      "basePersistDirectory": "\/tmp\/realtime\/basePersist",
      "rejectionPolicy": {
        "type": "serverTime"
      }
    }
  }
]
```

这是一个JSON数组，所以你可以给更多实时流到给定的节点。你放进相同程序的数量取决于精准的配置。
一般来讲，这个最好考虑每个实时流处理程序要求2-threads:1线程数据消费和聚合,线程增量持续和其他后台任务。

实时流规范有三部分，`dataSchema`, `IOConfig`,和 `tuningConfig`，我们下面开始讲。

### DataSchema

这个字段是必须的。

查阅[摄取](../ingestion/index.html)。
### IOConfig

这个字段是必须的。
|字段|类型|描述|必须|
|-----|----|-----------|--------|
|type|String|This should always be 'realtime'.|yes|
|firehose|JSON Object|Where the data is coming from. Described in detail below.|yes|
|plumber|JSON Object|Where the data is going. Described in detail below.|yes|

#### Firehose

查阅[Firehose](../ingestion/firehose.html)了解更多不同的firhoses信息。
#### Plumber

|字段|类型|描述|必须|
|-----|----|-----------|--------|
|type|String|This should always be 'realtime'.|no|

### 优化配置

这个优化配置是可选的，如果没有指定优化配置则使用默认参数。
|字段|类型|描述|必须|
|-----|----|-----------|--------|
|type|String|This should always be 'realtime'.|no|
|maxRowsInMemory|Integer|The number of rows to aggregate before persisting. This number is the post-aggregation rows, so it is not equivalent to the number of input events, but the number of aggregated rows that those events result in. This is used to manage the required JVM heap size. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists).|no (default == 500000)|
|windowPeriod|ISO 8601 Period String|The amount of lag time to allow events. This is configured with a 10 minute window, meaning that any event more than 10 minutes ago will be thrown away and not included in the segment generated by the realtime server.|no (default == PT10m)|
|intermediatePersistPeriod|ISO8601 Period String|The period that determines the rate at which intermediate persists occur. These persists determine how often commits happen against the incoming realtime stream. If the realtime data loading process is interrupted at time T, it should be restarted to re-read data that arrived at T minus this period.|no (default == PT10m)|
|basePersistDirectory|String|The directory to put things that need persistence. The plumber is responsible for the actual intermediate persists and this tells it where to store those persists.|no (default == java tmp dir)|
|versioningPolicy|Object|How to version segments.|no (default == based on segment start time)|
|rejectionPolicy|Object|Controls how data sets the data acceptance policy for creating and handing off segments. More on this below.|no (default=='serverTime')|
|maxPendingPersists|Integer|Maximum number of persists that can be pending, but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists).|no (default == 0; meaning one persist can be running concurrently with ingestion, and none can be queued up)|
|shardSpec|Object|This describes the shard that is represented by this server. This must be specified properly in order to have multiple realtime nodes indexing the same data stream in a [sharded fashion](#sharding).|no (default == 'NoneShardSpec'|
|buildV9Directly|Boolean|Whether to build v9 index directly instead of building v8 index and convert it to v9 format|no (default = false)|
|persistThreadPriority|int|If `-XX:+UseThreadPriorities` is properly enabled, this will set the thread priority of the persisting thread to `Thread.NORM_PRIORITY` plus this value within the bounds of `Thread.MIN_PRIORITY` and `Thread.MAX_PRIORITY`. A value of 0 indicates to not change the thread priority.|no (default = 0; inherit and do not override)|
|mergeThreadPriority|int|If `-XX:+UseThreadPriorities` is properly enabled, this will set the thread priority of the merging thread to `Thread.NORM_PRIORITY` plus this value within the bounds of `Thread.MIN_PRIORITY` and `Thread.MAX_PRIORITY`. A value of 0 indicates to not change the thread priority.|no (default = 0; inherit and do not override)|
|reportParseExceptions|Boolean|If true, exceptions encountered during parsing will be thrown and will halt ingestion. If false, unparseable rows and fields will be skipped. If an entire row is skipped, the "unparseable" counter will be incremented. If some fields in a row were parseable and some were not, the parseable fields will be indexed and the "unparseable" counter will not be incremented.|false|

启用线程前优先级设置，强烈建议用户阅读 [原始拉请求](https://github.com/druid-io/druid/pull/984)和其他关于适当使用`-XX:+UseThreadPriorities`的文档。
#### 拒绝策略

以下的政策是有效的：

* `serverTime` &ndash;为“当前时间”数据推荐的政策，是适合当前数据实时生成和摄取的。使用 `windowPeriod`只接受那些事件窗口内的查找前和查找后。
* `messageTime` &ndash;可以用于非“当前时间”只要数据是相对顺序。如果它们比最新时间戳`windowPeriod`事件少的话，事件将被拒绝。只有在一个事件发生后segmentGranularity和“windowPeriod”(手不会定期发生,除非你有源源不断的数据)才会切换。
* `none` &ndash;所有的事件都被接受。从不切换数据除非在配置Firehoses时调用shutdown()。

#### Sharding分区

Druid使用分区，或者有分区号的段，为了更有效率的处理大量的引入数据。在Druid，分区代表段一起覆盖基于`segmentGranularity`值的一个时间间隔。
例如，如果 `segmentGranularity`设置为“hour”,那么大量的分区可能用于存储数据。还可能出现分区沿着维度优化效率。
段被数据源、时间间隔和版本识别。分片段也是由分区号识别。
通常,每个碎片都有相同的版本,唯一地标识是一个不同的分区号。
在较小的数据情况下,分片是不必要的,可以设置为none(默认):
```json
    "shardSpec": {"type": "none"}
```

然而,在多个实时场景节点,`none`不太有用,因为它不能帮助扩展数据量(见下文)。注意,对于批处理索引服务,无需显式配置;分区是自动提供的。

Druid使用基于`shardSpec`分区设置你的配置。推荐选择，`linear`和`numberd`，下面有讲；其他类型对于内部Druid发展一直是有用的，但是不适用于生产设置。
记住，分区配置对配置Firehoses没有影响。例如，如果你设置分区号为0，这并不意味着Kafka firehose将仅从0主题分区消耗。
##### Linear

这个策略有以下优点：
* 当增加新节点时它不需要更新已存在节点的文件规范配置。
* 独特的碎片都是查询,不管分区编号是不是连续的(它允许查询分区0和2,即使分区1缺失)。

`schema`下配置`linear`：
```json
    "shardSpec": {
        "type": "linear",
        "partitionNum": 0
    }
```
            

##### Numbered

这种策略类似于`linear`,除了它不容忍非时序的分区编号(它将*不*允许查询分区0和2,如果分区1缺失)。它还需要显示设置分区的总数。
`schema`下配置`numbered`：
```json
    "shardSpec": {
        "type": "numbered",
        "partitionNum": 0,
        "partitions": 2
    }
```
     

##### 规模和冗余

`shardSpec`配置可以在不同的节点上通过相同的`partionNum`值创建冗余。

例如，如果实时节点1有：
```json
    "shardSpec": {
        "type": "linear",
        "partitionNum": 0
    }
```
            
实时节点2有：
```json
    "shardSpec": {
        "type": "linear",
        "partitionNum": 0
    }
```

那么这两个实时节点可以用相同的数据源，版本，时间间隔，和分区号存储段。在这个段查询数据代理将假设拥有相同的数据，而且查询只针对其中一个段。

`shardSpec`也可以帮助实现规模。为此,添加节点有不同的`partionNum`。如例子,如果实时节点3有:
```json
    "shardSpec": {
        "type": "linear",
        "partitionNum": 1
    }
```

那么它可以用相同的数据源,时间间隔,和在第一个两个节点版本,但是有不同的分区号存储段。
在这个段查询数据代理将假设从实时节点3一个段拥有 *不同的* 数据，而且查询将针对这一段的前两个节点。

你可以使用类似`numbered`类型。注意`none`类型本质上是`linear`有所有分区有固定的`分区号`0。

## 约束

下表总结了设置的规范文件实时子系统之间的约束。
|名称|影响|最小值|建议|
|----|------|-------|-----------|
|windowPeriod| When reading a row, events with timestamp older than now minus this window are discarded | time jitter tolerance | use this to reject outliers |
|segmentGranularity| Time granularity (minute, hour, day, week, month) for loading data at query time | equal to indexGranularity| more than queryGranularity|
|queryGranularity| Time granularity (minute, hour, day, week, month) for rollup | less than segmentGranularity| minute, hour, day, week, month |
|intermediatePersistPeriod| The max time (ISO8601 Period) between flushes of ingested rows from memory to disk | avoid excessive flushing | number of un-persisted rows in memory also constrained by maxRowsInMemory |
|maxRowsInMemory| The max number of ingested rows to hold in memory before a flush to disk | number of un-persisted post-aggregation rows in memory is also constrained by intermediatePersistPeriod | use this to avoid running out of heap if too many rows in an intermediatePersistPeriod |

通常情况下，希望用例有以下约束：`intermediatePersistPeriod ≤ windowPeriod < segmentGranularity`和 `queryGranularity ≤ segmentGranularity`
## 局限

### Kafka

独立的实时节点使用Kafka高水平的消费者,这强加了一些限制。

Druid复制段,这样逻辑上等价的数据段同时托管在N个节点。如果n - 1节点下降，数据仍将用于查询。
在实时节点，这个过程取决于保持逻辑上等价的每N个节点的数据段,这是不可能与标准Kafka消费者团体如果你的Kafka topic需要不止一个消费者(因为消费者不同的消费群体将分割数据不同)。
 
例如,假设您的主题是跨越Kafka分区1、2 & 3和2实时节点线性碎片规格1 & 2。实时节点都是在相同的消费群体。实时节点1可能从1&3分区消耗数据，实时节点2可能从分区2消耗数据。通过代理查询你的数据将产生正确的结果。

问题出现了,如果你想复制你的数据通过创建实时节点3和4。这些新的实时节点线性碎片规格1 & 2,他们将从Kafka使用不同的消费群体消费数据。在这种情况下,实时节点3可能消耗的数据分区1 & 2,和实时节点4可能消耗的数据分区2。
从Druid的角度来看,部分由实时节点1和3是相同的,和数据由实时节点2和4是相同的,尽管他们在阅读不同Kafka的分区。查询的数据将产生不一致的结果。

这是总是一个问题吗?不。如果您的数据是足够小,适合在一个Kafka分区,你可以复制不需要事件。否则,您可以运行实时节点没有复制。
###锁定

使用流拉一起摄入与实时节点批摄入可能会引入数据覆盖问题。例如,如果你当天,产生每小时段和运行当天每日批处理作业的数据,创建的片段批处理作业将最新版本比大多数实时摄取所产生的片段。
如果你的批处理作业是还没有完成的索引数据,由批处理作业创建的每日段，可以覆盖实时节点创建的最近的段。但是这样会丢失数据的一部分。

### 模式改变

独立的实时节点要求停止一个节点更新模式，然后再次启动该模式生效。
这很难管理规模,尤其是在多个分区。
### 日志管理

每个独立的实时节点都有自己的日志。诊断错误跨多个服务器，跨多个分区在规模内难以管理和跟踪。